{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CHat-Agent-Simulator (CHAS)","text":"<p>CHat-Agent-Simulator (CHAS) is an AI-powered diagnostic framework driven by Large Language Models (LLMs) and AI agents. It simulates thousands of edge-case scenarios to comprehensively evaluate chatbot agents. By stress-testing your agent from all angles in wide-range of complexity levels, CHAS helps identify potential failure points and provides detailed performance analysis to ensure reliable deployment.</p> <p>Features:</p> <ul> <li>\ud83d\udd2c Simulates thousands of realistic edge-case events tailored to your chatbot agent and system data schema</li> <li>\ud83e\udd16 Simulates user interactions with your agent based on generated events</li> <li>\ud83d\udcca Comprehensive report with evaluation metrics including success rates, policy &amp; tools failure points, complexity-level analysis</li> <li>\ud83d\udcaa Enables confident deployment of more reliable chatbot agents</li> </ul>"},{"location":"#why-chas","title":"Why CHAS?","text":"<p>Companies today face a critical challenge when deploying chatbot agents: balancing capability with reliability. Most organizations end up severely limiting their chatbots' capabilities (restricting them to read-only operations or minimal tool access) due to concerns about:</p> <ul> <li>Policy violations</li> <li>Inconsistent responses</li> <li>Unexpected edge cases</li> </ul> <p>This is where CHAS transforms the landscape. Instead of restricting your agent's capabilities due to uncertainty, CHAS empowers you to:</p> <ol> <li>Discover the Unknown: Generate thousands of edge-case scenarios that stress-test your agent from every angle</li> <li>Identify Failure Points: Proactively detect and understand where your agent might fail</li> <li>Optimize Capabilities: Find the optimal balance between functionality and reliability</li> <li>Deploy with Confidence: Release chat agents that are both powerful AND reliable</li> </ol> <p>CHAS turns the traditional \"restrict-by-default\" approach into a data-driven \"enable-with-confidence\" strategy. By thoroughly simulating and analyzing your agent's behavior, you can expand its capabilities while maintaining strict reliability standards.</p> <p>Don't limit your chatbot's potential because of what you don't know. Use CHAS to know exactly what your agent can handle, fix what it can't, and deploy with confidence.</p>"},{"location":"#how-chas-ai-pipeline-works","title":"\ud83d\ude80 How CHAS AI Pipeline Works","text":"<p>CHAS operates through an AI pipeline of components powered by LLMs and AI Agents that work together to test and evaluate chatbot agents. The pipeline is outlined below:</p> Step Description Illustration 1. Input Context Analyze the agent's context, including task description, tools, and database schema. 2. Flow and Policies Generation Decompose the agent's task into sub-flows and extract relevant policies. 3. Policies Graph Generation Create a weighted graph representing relationships between policies to identify complex scenarios. 4. Generate Dataset Event Create a dataset of scenario events with varying challenge scores for thorough testing. 5. Run Simulation Events Execute simulation events through an interactive dialog system, capturing performance insights. 6. Report (Comprehensive Evaluation) Generate a detailed evaluation report analyzing performance and failure points. <p>For more details, refer to the How it works (Explanation of AI Pipeline).</p>"},{"location":"#demo","title":"\ud83d\udd0d Demo","text":""},{"location":"#documentation","title":"\ud83d\udcd6 Documentation","text":"<ul> <li> <p>Installation &amp; Quick Start   Step-by-step setup instructions.</p> </li> <li> <p>How It Works   Detailed explanation of the CHAS pipeline.</p> </li> <li> <p>Architecture Guide   Overview of the system's main components.</p> </li> <li> <p>Running Examples   Running examples of various chat-agents, including airline agent and retail agent</p> </li> <li> <p>Understanding Different Checkpoints   An overview of the various types of checkpoints provided by CHAS for cost optimization.</p> </li> <li> <p>Creating New Custom Chat-Agent Environment   Comprehensive guide for setting up custom chat-agent environments, including domain-specific policies, database schemas, tools, and validators.</p> </li> <li> <p>Customizing Chat Agents   Guide for configuring custom chatbot agents, including LLM selection and LangGraph integration.</p> </li> </ul>"},{"location":"#quickstart","title":"QuickStart","text":"<p>Chat-Agent-Simulator (CHAS) requires <code>python == 3.10</code> </p> <p>Step 1 - Download the project</p> <pre><code>git clone git@github.com:plurai-ai/Chat-Agent-Simulator.git\ncd Chat-Agent-Simulator\n</code></pre> <p></p> <p>Step 2 - Install dependencies</p> <p>Use either Conda, pip, or pipenv, depending on your preference.</p> <p>Using Conda:</p> <pre><code>conda env create -f environment_dev.yml\nconda activate Chat-Agent-Simulator\n</code></pre> <p>Using pip: </p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Using pipenv:</p> <pre><code>pip install pipenv\npipenv install\n</code></pre> <p></p> <p>Step 3 - Configure your LLM API Key</p> <p>Edit the <code>config/llm_env.yml</code> file to set up your LLM configuration:</p> <pre><code>openai:\n  OPENAI_API_KEY: \"your-api-key-here\"\n  OPENAI_API_BASE: ''\n  OPENAI_ORGANIZATION: ''\n\nazure:\n  AZURE_OPENAI_API_KEY: \"your-api-key-here\"\n  AZURE_OPENAI_ENDPOINT: \"your-endpoint\"\n  OPENAI_API_VERSION: \"your-api-version\"\n\n</code></pre> <p></p> <p>Step 4 - Configure the simulator run parameters</p> <p>Before running the simulator, you need to configure the <code>config_default.yml</code> file located in the <code>config</code> folder. This file contains important settings for the simulator. Here's an overview of the main sections and their purposes:</p> <ul> <li> <p><code>environment</code>: Specifies paths for the prompt, tools, and data-scheme folders, as well as the task description. For example:   <code>yaml   environment:       prompt_path: 'examples/airline/wiki.md'       tools_folder: 'examples/airline/tools/agent_tools.py'       database_folder: 'examples/airline/data_scheme'</code></p> </li> <li> <p><code>description_generator</code>: Contains settings for various components of the description generation process, including flow extraction, policies extraction, edge configuration, and description refinement.</p> </li> <li> <p><code>event_generator</code>: Configures the event generation process, including the LLM to use and worker settings.</p> </li> <li> <p><code>dialog_manager</code>: Sets up the simulator dialog management system, including user parsing mode, memory path, and LLM configurations for user simulation and chat.</p> </li> <li> <p><code>dataset</code>: Defines parameters for dataset generation, such as difficulty levels, number of samples, and cost limits in $.</p> </li> </ul> <p>Key points to configure:</p> <ol> <li> <p>Update file paths in the <code>environment</code> section to match your project structure:    <code>yaml    prompt_path: 'examples/airline/wiki.md'  # Path to your agent's wiki/documentation    tools_folder: 'examples/airline/tools'   # Path to your agent's tools    database_folder: 'examples/airline/data' # Path to your data schema</code></p> </li> <li> <p>Adjust LLM configurations throughout the file:    <code>yaml    llm:        type: 'openai'  # or 'azure'        name: 'gpt-4o'   # or other model names as gpt-4o-mini</code></p> </li> <li> <p>Configure worker settings based on your system's capabilities. This parameter is highly important in cases of rate-limit responses:    <code>yaml    num_workers: 3    # Number of parallel workers    timeout: 10       # Timeout in seconds</code></p> </li> <li> <p>Set appropriate cost limits to control API usage:    <code>yaml    cost_limit: 30    # In dollars, for simulator dialog manager</code></p> </li> </ol> <p>For a complete example configuration, check the <code>config/config_default.yml</code> file in the repository.</p> <p></p> <p>Step 5 - Run the Simulator</p>"},{"location":"#running-the-simulation-script","title":"Running the Simulation Script","text":"<p>The <code>run.py</code> script is designed to execute a simulation using a specified configuration and dataset. Follow the steps below to run the script:</p>"},{"location":"#usage","title":"Usage","text":"<p>To run the script, use the following command:</p> <pre><code>python run.py --output_path &lt;output_path&gt; [--config_path &lt;config_path&gt;] [--dataset &lt;dataset&gt;]\n</code></pre>"},{"location":"#arguments","title":"Arguments","text":"<ul> <li><code>--output_path</code>: (Required) Specify the path where the output file will be saved.</li> <li><code>--config_path</code>: (Optional) Specify the path to the configuration file. Defaults to <code>config/config_default.yml</code>.</li> <li><code>--dataset</code>: (Optional) Specify the dataset name. If set to <code>'latest'</code>, the latest dataset will be loaded. Defaults to <code>'latest'</code>.</li> </ul>"},{"location":"#example","title":"Example","text":"<pre><code>python run.py --output_path ./examples/airline/output/run_1 --config_path ./config/config_airline.yml \n</code></pre> <p>This command will run the simulation using the specified configuration and dataset, and save the results to <code>../output/exp1</code>.</p> <p></p> <p>Step 6 - Analyze Simulator Results</p> <p>After the simulation completes, you can find the results in the specified output path directory with the following structure:</p> <pre><code>experiments/\n\u251c\u2500\u2500 dataset__[timestamp]__exp_[n]/    # Experiment run folder\n\u2502   \u251c\u2500\u2500 experiment.log                # Detailed experiment execution logs\n\u2502   \u251c\u2500\u2500 config.yaml                   # Configuration used for this run\n\u2502   \u251c\u2500\u2500 prompt.txt                    # Prompt template used\n\u2502   \u251c\u2500\u2500 memory.db                     # Dialog memory database\n\u2502   \u2514\u2500\u2500 results.csv                   # Evaluation results and metrics\n\u2502\ndatasets/\n\u251c\u2500\u2500 dataset__[timestamp].pickle       # Generated dataset snapshot\n\u2514\u2500\u2500 dataset.log                       # Dataset generation logs\n\u2502\npolicies_graph/\n\u251c\u2500\u2500 graph.log                         # Policy graph generation logs\n\u2514\u2500\u2500 descriptions_generator.pickle     # Generated descriptions and policies\n</code></pre> <p>Each experiment run creates a timestamped folder containing all relevant files for analysis and reproduction. The results.csv file contains the comprehensive evaluation metrics discussed in Step #6 of the System Overview.</p> <p>To visualize the simulation results using streamlit, run:</p> <pre><code>streamlit run simulator/visualization/Simulator_Visualizer.py\n</code></pre> <p>This will launch a Streamlit dashboard showing detailed analytics and visualizations of your simulation results.</p> <p></p>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li>[x] Beta Release</li> <li>[ ] Expand Integration with More Agent Platforms<ul> <li>[ ] CrewAI</li> <li>[ ] AutoGen</li> </ul> </li> <li>[ ] Enable Event Generation from Existing Databases</li> <li>[ ] Implement API Integration for External Chatbot Agents</li> <li>[ ] Add Personality Dimensions to User Agents</li> <li>[ ] Optimize Chatbot Performance Using Simulator Diagnostics (Access now by joining our premium program)<ul> <li>[ ] System Prompt Optimization</li> <li>[ ] Tools Optimization</li> <li>[ ] Graph structure Optimization</li> </ul> </li> </ul> <p>Join our Discord community to shape our roadmap!</p>"},{"location":"#community-contributing","title":"\ud83d\ude80 Community &amp; Contributing","text":"<p>Your contributions are greatly appreciated! If you're eager to contribute, kindly refer to our Contributing Guidelines) for detailed information. We\u2019re particularly keen on receiving new examples and environments to enrich the project.</p> <p>If you wish to be a part of our journey, we invite you to connect with us through our Discord Community. We're excited to have you onboard! </p>"},{"location":"#disclaimer","title":"\ud83d\udee1 Disclaimer","text":"<p>The Chat-Agent-Simulator (CHAS) project is provided on an \"as-is\" basis without any guarantees or warranties, either expressed or implied. By using this software, you acknowledge that:</p> <ul> <li>The developers and contributors are not responsible for any damages or losses that may arise from the use or inability to use the software.</li> <li>The software is intended for testing and evaluation purposes only and should not be used in production environments without thorough testing and validation.</li> <li>The accuracy, reliability, and performance of the software are not guaranteed, and users should exercise caution and perform their own assessments before relying on the software for critical applications.</li> <li>Any modifications or customizations made to the software are the sole responsibility of the user, and the developers are not liable for any issues that may result from such changes.</li> </ul> <p>By downloading or using the Chat-Agent-Simulator, you agree to these terms and conditions. If you do not agree, please refrain from using the software.</p>"},{"location":"#citation","title":"Citation","text":"<p>TODO</p>"},{"location":"#license","title":"License","text":"<p>This framework is licensed under the Apache License, Version 2.0.</p> <p>Under this license, you are free to use, modify, and distribute the software, provided that you include a copy of the license and any notices in any copies or substantial portions of the software. </p> <p>The software is provided \"as is\", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, and noninfringement. See the Apache License, Version 2.0 for the full text and more details.</p>"},{"location":"#open-analytics","title":"\ud83d\udd0d Open Analytics","text":"<p>We collect basic usage metrics to better understand our users' needs and improve our services. As a transparent startup, we are committed to open-sourcing all the data we collect. Plurai does not track any information that can identify you or your company. You can review the specific metrics we track in the code.</p> <p>If you prefer not to have your usage tracked, you can disable this feature by setting the <code>PLURAI_DO_NOT_TRACK</code> flag to true.</p>"},{"location":"#support-contact-us","title":"\u2709\ufe0f Support / Contact us","text":"<ul> <li>Join our Community for discussions, updates and announcements Community Discord</li> <li>Our email: \u202bchas@plurai.ai\u202c</li> <li>GitHub Issues for bug reports and feature requests</li> </ul>"},{"location":"How_it_Works/architecture/","title":"Architecture System Overview","text":"<p>The CHAS system pipeline consists of three main components:</p> <ol> <li>Event Generation</li> <li>Input: Database schema and either:<ul> <li>Chatbot system prompt, or</li> <li>Company policy documentation</li> </ul> </li> <li> <p>Output:</p> <ul> <li>Policy graph representation</li> <li>Generated events with varying complexity levels, including:</li> <li>Scenario descriptions</li> <li>User requests</li> <li>Initial database states</li> </ul> </li> <li> <p>Dialog Simulation</p> </li> <li>Input: Generated events and configured agents</li> <li> <p>Output: </p> <ul> <li>Complete conversation transcripts</li> <li>Tool usage logs</li> <li>Agent reasoning traces</li> </ul> </li> <li> <p>Fine-Grained Analysis</p> </li> <li>Input: Dialog transcripts and policy requirements</li> <li>Output:<ul> <li>Detailed performance metrics</li> <li>Policy compliance analysis</li> <li>Complexity-based evaluation reports</li> </ul> </li> </ol>"},{"location":"How_it_Works/architecture/#1-event-generation","title":"1. Event Generation","text":"<p>The CHAS Event Generator creates realistic chatbot interactions through a multi-stage pipeline that transforms high-level policies into concrete scenarios with corresponding database states. The process consists of three main components:</p>"},{"location":"How_it_Works/architecture/#11-policy-analysis-graph-construction","title":"1.1 Policy Analysis &amp; Graph Construction","text":"<p>The <code>DescriptionGenerator</code> class handles the initial policy analysis through these steps:</p> <ol> <li>Flow Extraction: Breaks down the system prompt into distinct flows</li> <li>Policy Extraction: Analyzes each flow to identify individual policies and assigns complexity scores</li> <li>Graph Construction: Creates a weighted graph where:</li> <li>Nodes: Individual policies with complexity scores</li> <li>Edges: Weighted connections (1-10) indicating the likelihood of two policies appearing together in the same task</li> <li>Edge weights are determined through LLM evaluation of policy pairs</li> </ol>"},{"location":"How_it_Works/architecture/#12-event-generation-pipeline","title":"1.2 Event Generation Pipeline","text":""},{"location":"How_it_Works/architecture/#121-description-generation","title":"1.2.1 Description Generation","text":"<p>The system generates event descriptions through: 1. Sampling policies based on target complexity:    - Starts with a random policy node    - Performs weighted random walks based on edge weights    - Continues until reaching desired complexity threshold 2. Converting selected policies into natural language scenarios 3. Generating expected chatbot behaviors 4. (Optional) Refining expected behaviors through feedback iterations</p>"},{"location":"How_it_Works/architecture/#122-symbolic-representation","title":"1.2.2 Symbolic Representation","text":"<p>The <code>EventsGenerator</code> class transforms descriptions into concrete events by: 1. Creating symbolic representations of entities and relationships 2. Defining database constraints based on policies 3. Converting symbolic representations into actual database states through:    - Table-specific insertion tools    - Validation of referential integrity    - Parallel processing of multiple events</p> <p></p>"},{"location":"How_it_Works/architecture/#dialog-simulation-architecture","title":"Dialog Simulation Architecture","text":"<p>The Dialog Simulation system orchestrates conversations between simulated users and chatbots. The system consists of two main components: the Dialog Graph and Dialog Manager.</p>"},{"location":"How_it_Works/architecture/#dialog-graph","title":"Dialog Graph","text":"<p>The Dialog Graph implements a state machine that manages the conversation flow between the simulated user and chatbot. Key features include:</p> <ul> <li>State Management: Uses <code>DialogState</code> to track:</li> <li>User and chatbot message history</li> <li>User thoughts and reasoning</li> <li>Critique feedback</li> <li> <p>Stop signals for conversation termination</p> </li> <li> <p>Node Types:</p> </li> <li>User Node: Simulates user responses and tracks reasoning</li> <li>Chatbot Node: Handles chatbot responses and tool interactions</li> <li> <p>Critique Node: Evaluates conversation adherence to policies</p> </li> <li> <p>Flow Control:</p> </li> <li>Manages message passing between participants</li> <li>Handles conversation termination conditions</li> <li>Supports conversation feedback loops through critique evaluation</li> </ul>"},{"location":"How_it_Works/architecture/#dialog-manager","title":"Dialog Manager","text":"<p>The Dialog Manager controls the execution of dialogs and provides the infrastructure for running simulations. Key features include:</p> <ul> <li>Configuration Management:</li> <li>Configures LLM models for user, chatbot, and critique agents</li> <li>Sets up environment tools and schemas</li> <li> <p>Manages prompt templates and parsing functions</p> </li> <li> <p>Execution Modes:</p> </li> <li>Synchronous execution (<code>run</code>)</li> <li>Asynchronous execution (<code>arun</code>)</li> <li> <p>Batch processing of multiple events (<code>run_events</code>)</p> </li> <li> <p>Memory Management:</p> </li> <li>SQLite-based conversation storage</li> <li>Tracks tool calls and their outputs</li> <li>Maintains conversation history</li> </ul>"},{"location":"How_it_Works/architecture/#simulation-flow","title":"Simulation Flow","text":"<ol> <li>The Dialog Manager initializes the environment and agents</li> <li>For each event:</li> <li>User agent receives event details and expected behaviors</li> <li>Conversation alternates between user and chatbot</li> <li>Critique agent evaluates responses against policies</li> <li>Conversation continues until success or policy violation</li> </ol>"},{"location":"How_it_Works/architecture/#key-features","title":"Key Features","text":"<ul> <li>Policy Enforcement: Built-in critique system to evaluate chatbot adherence to defined policies</li> <li>Parallel Processing: Support for parallel execution of multiple conversations</li> <li>Extensible Architecture: Modular design allowing for custom LLMs, tools, and evaluation criteria</li> <li>Comprehensive Logging: Detailed tracking of conversations, tool usage, and agent reasoning</li> </ul>"},{"location":"How_it_Works/architecture/#3-fine-grained-analysis","title":"3. Fine-Grained Analysis","text":"<p>The Dialog critique component performs a detailed evaluation of the conversation by analyzing:</p> <ol> <li>The user-chatbot dialog history</li> <li>The chatbot's system prompt</li> <li>The termination reason provided by the user agent</li> </ol> <p>The critique process follows these steps:</p> <ol> <li>Termination Validation: Verifies if the stated reason for dialog termination is accurate</li> <li>If incorrect: Provides feedback through <code>critique_feedback</code> and continues the dialog</li> <li> <p>If correct: Proceeds with policy analysis</p> </li> <li> <p>Multi-Level Policy Coverage Analysis: </p> </li> <li>Identifies which event policies were tested during the conversation</li> <li>Determines which policies were violated (if any)</li> <li>Evaluates policies across different complexity levels, ranging from straightforward tasks to highly complex and nuanced edge-case scenarios  </li> <li>Report Generation: Creates a detailed performance assessment based on the above analysis, with insights categorized by policies and complexity level</li> </ol>"},{"location":"How_it_Works/how-it-works/","title":"How Chat-Agent-Simulator works","text":"<p>Chat-Agent-Simulator (CHAS) operates through a sophisticated AI pipeline powered by Large Language Models (LLMs) and Langraph agents. This document outlines the step-by-step optimization process flows.</p>"},{"location":"How_it_Works/how-it-works/#pipeline-overview","title":"Pipeline Overview","text":""},{"location":"How_it_Works/how-it-works/#1-input-context-analysis","title":"1. Input Context Analysis","text":"<p>The process begins with a comprehensive analysis of your agent's context, including: - Task descriptions - Available tools - Database schema</p> <p>This initial analysis establishes the foundation for understanding the agent's intended behavior and operational boundaries.</p> <p></p>"},{"location":"How_it_Works/how-it-works/#2-flow-and-policies-generation","title":"2. Flow and Policies Generation","text":"<p>The system automatically: - Decomposes the agent's primary task into sub-flows - Extracts relevant policies and guidelines - Identifies potential edge cases and interaction patterns</p> <p></p>"},{"location":"How_it_Works/how-it-works/#3-policies-graph-generation","title":"3. Policies Graph Generation","text":"<p>CHAS creates a weighted graph structure where: - Nodes represent individual policies - Edges indicate relationships between policies - Edge weights reflect the challenge score of implementing connected policies together</p> <p>This graph helps identify complex interaction scenarios requiring testing.</p> <p></p>"},{"location":"How_it_Works/how-it-works/#4-dataset-event-generation","title":"4. Dataset Event Generation","text":"<p>The system generates comprehensive test scenarios including: - Scenario Content: Specific user interaction cases - Expected Output: Desired agent behavior based on policies - Data Ingestion: Required database records and state prerequisites</p> <p>Each scenario is assigned varying challenge scores to ensure thorough testing across complexity levels.</p> <p></p>"},{"location":"How_it_Works/how-it-works/#5-simulation-execution","title":"5. Simulation Execution","text":"<p>CHAS runs generated scenarios through an interactive dialog system, capturing: - Event execution details - Complete dialog logs - Simulated user interactions - Agent thought processes - Performance metrics</p> <p></p>"},{"location":"How_it_Works/how-it-works/#6-comprehensive-evaluation","title":"6. Comprehensive Evaluation","text":"<p>The final output includes detailed analysis across multiple dimensions: - Complexity level performance breakdown - Policy interaction success/failure patterns - Tool usage effectiveness - Specific failure point identification</p> <p></p>"},{"location":"How_it_Works/how-it-works/#key-components","title":"Key Components","text":"<p>The framework is designed with modularity and adaptability in mind, allowing for: - Easy extension to support new input contexts - Integration with different chat agents - Customizable evaluation metrics - Scalable testing scenarios</p> <p>This architecture ensures that CHAS can evolve to meet changing testing needs while maintaining comprehensive coverage of potential edge cases and failure points.</p>"},{"location":"contribution/contributing/","title":"Contributing to Chat-Agent-Simualtor","text":"<p>Thank you for considering contributing to Chat-Agent-Simualtor! We deeply appreciate your interest in improving our project.</p>"},{"location":"contribution/contributing/#bug-fixes-and-documentation-enhancements","title":"Bug Fixes and Documentation Enhancements","text":"<p>Bug fixes and documentation improvements, including compelling examples and use cases, greatly benefit our project. If you encounter any bugs or identify areas where the documentation could be strengthened, please do not hesitate to submit a pull request (PR) containing your proposed changes.</p>"},{"location":"contribution/contributing/#feature-requests","title":"Feature Requests","text":"<p>For significant feature additions, we encourage you to open an issue on GitHub. Additionally, we invite you to join our Discord community and engage in discussions about the feature in the #features-requests channel. This collaborative environment enables us to delve deeper into the proposed features and foster meaningful dialogue.</p> <p>We value your contributions and look forward to working together to enhance Chat-Agent-Simualtor!</p>"},{"location":"customization/checkpoints/","title":"Checkpoints and Cost Saving and Monitoring","text":"<p>The CHAS system is optimized for cost efficiency and provides multiple levels of checkpoints to reduce expenses.</p> <p>This document outlines the different types of checkpoints and their locations within the pipeline.</p>"},{"location":"customization/checkpoints/#policies-graph-checkpoint","title":"Policies Graph Checkpoint","text":"<p>When initializing the <code>SimulatorExecutor</code>, the system searches for the <code>DescriptionGenerator</code> checkpoint at:  </p> <pre><code>&lt;args.output_path&gt;/policies_graph/descriptions_generator.pickle\n</code></pre> <p>If the checkpoint exists, the <code>descriptions_generator</code> (which contains the policies graph) will be loaded. If it does not exist, the system will generate the graph. During execution, the system automatically saves the <code>descriptions_generator</code> to this path.</p> <p>As a result, it will be loaded by default in subsequent runs unless the <code>output_path</code> is changed.</p>"},{"location":"customization/checkpoints/#dataset-events-checkpoints","title":"Dataset Events Checkpoints","text":"<p>Generating dataset events is a computationally expensive process. It is recommended to run it once and reuse the same dataset for all experiments to ensure consistency across experiments. You can specify the dataset name using the <code>--dataset</code> argument. The dataset will be saved at the following location:</p> <pre><code>&lt;args.output_path&gt;/datasets/&lt;dataset_name&gt;.pickle\n</code></pre> <p>During each iteration of size <code>mini_batch_size</code> (as defined in the configuration file), all generated events up to that point will be saved as checkpoints. This allows you to resume the dataset generation from the last mini-batch if it is interrupted.</p> <p>By default, the <code>--dataset</code> argument is set to <code>latest</code>, which will automatically load the most recently generated dataset.</p> <p>Additionally, you can set a <code>cost_limit</code> (in dollars) by defining the <code>cost_limit</code> variable in the configuration file. Note that this feature may not be supported by all models.</p>"},{"location":"customization/checkpoints/#experiment-checkpoint","title":"Experiment Checkpoint","text":"<p>Running the simulator on all events can be both costly and time-consuming. It is important to note that, unlike previous checkpoints, a new experiment is generated by default at each run.</p> <p>The experiment name can be specified using the <code>--experiment</code> variable. If not provided, it will default to <code>exp_{i}</code>, where <code>i</code> is a sequential number. All experiment dumps will be saved in the following folder:</p> <pre><code>&lt;args.output_path&gt;/experiments/&lt;dataset_name&gt;__&lt;experiment_name&gt;\n</code></pre> <p>The simulator results are saved after every <code>mini_batch_size</code> (as defined in the configuration file). If the run is interrupted and you want to resume it, you need to set the <code>--experiment</code> variable to the <code>experiment_name</code>.</p> <p>Additionally, you can define a <code>cost_limit</code> (in dollars) in the configuration file by setting the <code>cost_limit</code> variable. Note that this feature may not be supported by all models.</p>"},{"location":"customization/custom_chatbot/","title":"Custom Chatbot Setup","text":"<p>This guide explains how to configure the chatbot agent in the simulator. Currently, two types of customizations are supported: 1. Modifying the LLM in the default tool-calling chatbot agent. 2. Providing your own LangGraph-based chatbot agent.</p> <p>In the future, additional integrations will be available for platforms like crewAI and AutoGEN.</p>"},{"location":"customization/custom_chatbot/#prerequisites","title":"Prerequisites:","text":"<p>Before customizing the chatbot, ensure the environment is set up by following the instructions here.</p>"},{"location":"customization/custom_chatbot/#tool-calling-agent","title":"Tool-Calling Agent","text":"<p>If you're using a basic tool-calling agent, the only required modification (apart from environment customization) is selecting the appropriate LLM. CHAS supports all LangChain-compatible tool supported LLMs. Simply define the LLM type and name in the configuration file according to your chatbot settings:  </p> <pre><code>llm_chat:\n    type: ''\n    name: ''\n</code></pre>"},{"location":"customization/custom_chatbot/#setting-the-system-prompt","title":"Setting the System Prompt","text":"<p>By default, the agent uses the system prompt specified in the environment settings, along with the welcome message: \"Hello! \ud83d\udc4b I'm here to help with any request you might have.\" </p> <p>To customize the system prompt or the welcome message, you need to define <code>dialog_manager.chatbot_initial_messages</code> before running the simulator. The variable should be a <code>List[BaseMessage]</code> (a list of LangChain base messages).  </p> <p>Here\u2019s an example of how to modify the <code>run.py</code> main function to use a custom prompt:  </p> <pre><code>from langchain_core.messages import AIMessage, SystemMessage\n\n# Initialize the simulator executor with the environment configuration\nexecutor = SimulatorExecutor(config, args.output_path)\n\n# Load the dataset (default is the latest). To load a specific dataset, pass its path.\nexecutor.load_dataset(args.dataset)\n\n# Define the initial messages (system prompt and welcome message)\nmessages = [\n    SystemMessage(content='Enter the chatbot system message here'),\n    AIMessage(content=\"Hello, how can I help you?\")\n]\n\n# Update the default initial messages\nexecutor.dialog_manager.chatbot_initial_messages = messages\n\n# Run the simulation on the dataset\nexecutor.run_simulation()\n</code></pre>"},{"location":"customization/custom_chatbot/#general-langgraph-agent","title":"General LangGraph Agent","text":"<p>If your chatbot is based on LangGraph, you can customize the chatbot agent to fit your graph architecture. Ensure that your graph adheres to the following requirements:</p> <ul> <li> <p>The graph state must include the following (additional variables should be optional): <code>python messages: Annotated[list[AnyMessage]] args: dict[Any]</code></p> </li> <li> <p>The <code>messages</code> list should contain all interactions between the user and the chatbot. If internal calls made by the chatbot need to be tracked (e.g., it was part of the provided policies), this can be done using tool-calling messages:</p> </li> </ul> <pre><code>from langchain_core.messages import AIMessage, ToolMessage\n\nmessages += [\n    AIMessage(\n        content=\"\",\n        tool_calls=[\n            {'name': 'get_product_details', 'args': {'product_id': 'P2'}, 'id': '1', 'type': 'tool_call'}\n        ]\n    ),\n    ToolMessage(content='The product is not available currently in the store')\n]\n ```\n\n- If certain tools require database access, the database will be in the graph state within the `args` variable under the `data` key.\nIf the tool is properly defined (see [custom tools](./custom_environment.md#tools_file) with the `data` variable decorated by `InjectedState`, the agent should include the variable before invoking the tool. Below is an example of how this is handled in the base tool graph implementation: [tool graph](../simulator/agents_graphs/langgraph_tool.py):\n````python\nfor tool_call in state[\"messages\"][-1].tool_calls:\n    tool = self.tools_by_name[tool_call[\"name\"]]\n    all_tool_args = list(inspect.signature(tool.func).parameters)\n    function_args = copy.deepcopy(tool_call[\"args\"])\n    if state['args'] is not None:\n        function_args.update({k: v for k, v in state['args'].items()\n                              if (k in all_tool_args) and (k not in function_args)})\n        observation = tool.func(**function_args)\n ````\n\nIf you have a LangGraph compiled graph that satisfies these conditions, set it in `dialog_manager.chatbot` before running the simulator. Here's an example of how to modify the `run.py` main function to use a custom graph:\n\n```python\n# Initialize the simulator executor with the environment configuration\nexecutor = SimulatorExecutor(config, args.output_path)\n\n# Load the dataset (default is the latest). To load a specific dataset, pass its path.\nexecutor.load_dataset(args.dataset)\n\n# Set the chatbot graph\nexecutor.dialog_manager.chatbot = chatbot\n\n# Run the simulation on the dataset\nexecutor.run_simulation()\n</code></pre> <p>To provide a system prompt to the agent, configure the <code>initial_messages</code> variable as described here.</p>"},{"location":"customization/custom_environment/","title":"Custom Chat-Agent Environment Setup","text":"<p>This guide provides instructions for setting up a custom environment.</p> <p>You should create a new config_env.yml file and define there the chatbot environment variables</p> <pre><code>environment:\n    prompt_path:  # Path to prompt\n    tools_file: # Path to a python script that include all the tools functions \n    database_folder: # Path to database folder\n    database_validators: # Optional! Path to the file with the validators functions\n</code></pre> <p>After defining properly all the variables, you can run the simulator on the new custom environment:</p> <pre><code>python run.py \\\n    --output_path PATH     # Required: Directory where output files will be saved\n    --config_path PATH     # Optional: Path to config_env.yml (default: ./config_default.yml)\n    --dataset NAME         # Optional: Dataset name to use (default: 'latest')\n</code></pre>"},{"location":"customization/custom_environment/#environment-variables","title":"Environment Variables","text":""},{"location":"customization/custom_environment/#prompt_path","title":"<code>prompt_path</code>","text":"<p>This variable specifies the path to a prompt file. The file should be a simple text file (<code>.txt</code>) or a markdown file (<code>.md</code>) containing the desired prompt. The prompt doesn't have to be the exact chatbot system prompt, it can also be a document that describes the list of policies that should be tested. In this case the chatbot system prompt should be also provided (see tool chatbot modification).</p> <p>Example of prompt file: For a complete example of a prompt file, see the airline chat-agent system prompt.</p>"},{"location":"customization/custom_environment/#database_folder","title":"<code>database_folder</code>","text":"<p>This variable specifies the path to a folder containing CSV files. Each CSV file represents a database table used by the system and must include at least one row as an example. It is recommended to provide meaningful and indicative names for the columns in each CSV file.</p> <p>Example of database_folder: For a complete example of a database folder, see the airline chat-agent database scheme folder.</p> <p>The folder should contain CSV files that define your database tables. Here's an example structure from an airline booking system:</p> <p>flights.csv</p> flight_number origin destination scheduled_departure_time_est scheduled_arrival_time_est dates HAT001 PHL LGA 06:00:00 07:00:00 {\"2024-05-16\": {\"status\": \"available\", \"available_seats\": {\"basic_economy\": 16, \"economy\": 10, \"business\": 13}, \"prices\": {\"basic_economy\": 87, \"economy\": 122, \"business\": 471}}} <p>reservations.csv</p> reservation_id user_id origin destination flight_type cabin flights passengers payment_history created_at total_baggages nonfree_baggages insurance 4WQ150 chen_jackson_3290 DFW LAX round_trip business [{\"origin\": \"DFW\", \"destination\": \"LAX\", \"flight_number\": \"HAT170\", \"date\": \"2024-05-22\"}] [{\"first_name\": \"Chen\", \"last_name\": \"Jackson\", \"dob\": \"1956-07-07\"}] [{\"payment_id\": \"gift_card_3576581\", \"amount\": 4986}] 2024-05-02 03:10:19 5 0 no <p>users.csv</p> user_id name address email dob payment_methods saved_passengers membership reservations mia_li_3668 {\"first_name\": \"Mia\", \"last_name\": \"Li\"} {\"address1\": \"975 Sunset Drive\", \"city\": \"Austin\", \"country\": \"USA\"} mia.li@example.com 1990-04-05 {\"credit_card_4421486\": {\"source\": \"credit_card\", \"last_four\": \"7447\"}} [] gold [\"NO6JO3\"]"},{"location":"customization/custom_environment/#tools_file-optional","title":"<code>tools_file</code> (optional)","text":"<p>This variable specifies the path to a python script containing all the agent tool functions. </p> <p>The tool functions must be implemented using one of the following approaches: - Using LangChain's <code>@tool</code> decorator: LangChain Tool Decorator Guide - Using LangChain's <code>StructuredTool</code>: LangChain StructuredTool Guide</p> <p>If the tool needs to access the database you should add to the function a variable 'data', and use langchain InjectedState class.  In the following way:</p> <pre><code>def tool_function(data: Annotated[dict, InjectedState(\"dataset\")]):\n</code></pre> <p>The data variable will contain a dictionary of dataframe, where the name is the table name (according to the csv file name in the database folder).</p> <p>Optionally, you can define a tool schema by creating a variable named <code>&lt;function_name&gt;_schema</code>. If no schema variable is provided, the system will infer the schema automatically.</p> <p>Example of a valid <code>tools_file</code>: See airline chat-agent tools python script for reference.</p>"},{"location":"customization/custom_environment/#database_validators-optional","title":"<code>database_validators</code> (optional)","text":"<p>Data validators are crucial components of the system. These functions guide the database generation pipeline, ensuring data integrity and consistency. They are particularly important when dealing with duplicate information across different tables, as they allow for consistency checks.</p> <p>The <code>database_validators</code> variable specifies the path to a Python script that contains the data validation functions.</p> <p>To define a validation function, use the <code>@validator</code> decorator and specify the table to which the function applies.</p> <p>Example Validator Function:</p> <pre><code>from simulator.utils.file_reading import validator\n\n@validator(table='users')\ndef user_id_validator(new_df, dataset):\n    if 'users' not in dataset:\n        return new_df, dataset\n    users_dataset = dataset['users']\n    for index, row in new_df.iterrows():\n        if row['user_id'] in users_dataset.values:\n            error_message = f\"User id {row['user_id']} already exists in the users data. You should choose a different user id.\"\n            raise ValueError(error_message)\n    return new_df, dataset\n</code></pre> <ul> <li>The <code>@validator</code> decorator requires the table name as an argument.</li> <li>The validator function is applied before new data is inserted into the database.</li> </ul> <p>For a complete example of validators in action, see the airline booking system validators at airline chat-agent database validators python script. This example includes validators for: - User ID validation (preventing duplicate users) - Flight ID validation (ensuring unique flight numbers) - Flight validation (verifying flight details in reservations) - User validation (maintaining consistency between reservations and user data)</p>"},{"location":"examples/airline/","title":"Step-by-Step Guide to Running the Airline Agent","text":""},{"location":"examples/airline/#step-1-configure-the-simulator-run-parameters","title":"Step 1 - Configure the Simulator Run Parameters","text":"<p>Edit the <code>config/config_airline.yml</code> file to set the paths for the airline agent. Here\u2019s an example configuration:</p> <pre><code>environment:\n    prompt_path: 'examples/input/airline/wiki.md'  # Path to your agent's wiki/documentation\n    tools_file: 'examples/input/airline/tools/agent_tools.py'   # Path to your agent's tools \n    database_folder: 'examples/input/airline/data' # Path to your data schema\n    database_validators: 'examples/airline/input/validators/data_validators.py' # Optional! Path to the file with the validators functions\n</code></pre> <p>The <code>examples/input/airline</code> folder contains the following structure:</p> <pre><code>examples/\n\u2514\u2500\u2500 airline/\n    \u251c\u2500\u2500 wiki.md                       # Prompt for the airline agent\n    \u251c\u2500\u2500 tools/                        # Directory containing all the agent tools \n    \u2502   \u251c\u2500\u2500 agent_tools.py            # Python script containing all the agent tools\n    \u2502   \u251c\u2500\u2500 book_reservation_tool.py  # Python script containing the book reservation tool\n    \u2502   \u251c\u2500\u2500 update_reservation_baggages.py  # Tool to update baggage information\n    \u2502   \u251c\u2500\u2500 update_reservation_passengers.py  # Tool to update passenger information\n    \u2502   \u251c\u2500\u2500 cancel_reservation.py      # Tool to cancel a reservation\n    \u2514\u2500\u2500 data_scheme/                  # Directory containing data schema for the agent\n        \u251c\u2500\u2500 flights.json              # Flights data scheme and example \n        \u2514\u2500\u2500 reservation.json          # Reservation data scheme and example\n        \u2514\u2500\u2500 users.json                # Users data scheme and example\n</code></pre>"},{"location":"examples/airline/#step-2-run-the-simulator-and-understand-the-output","title":"Step 2 - Run the Simulator and Understand the Output","text":""},{"location":"examples/airline/#running-the-simulation","title":"Running the Simulation","text":"<p>Execute the simulator using:</p> <pre><code>python run.py --config ./config/config_airline.yml --output_path ./examples/airline/output/run_1 \n</code></pre>"},{"location":"examples/airline/#understanding-the-descriptor-generator-output","title":"Understanding the Descriptor Generator Output","text":"<p>The simulator processes your input in several stages:</p> <p>2.0. Task Description Generation    - Automatically inferred from the prompt (can be manually specified in <code>config_airline.yml</code>)    - Defines the chatbot's role as an airline agent handling reservations within policy constraints</p> <p>2.1. Flow Extraction    The system identifies four main flows:    - Book flight    - Modify flight    - Cancel flight    - Refund</p> <p>2.2. Policy Extraction    Each flow has associated policies. Examples:</p> Flow Policy Example Category Challenge Score Book Flight Agent must obtain user ID, trip type, origin, and destination Knowledge extraction 2 Modify Flight All reservations can change cabin without changing flights Company policy 2 Cancel Flight Cancellation allowed within 24h of booking or airline cancellation Logical Reasoning 4 Refund Compensation available for eligible members based on status/insurance Company policy 3 <p>2.3. Relations Graph Generation    - Creates a network of policy relationships    - Nodes: Individual policies    - Edges: Policy relationships    - Weights: Combined challenge scores</p> <p>All descriptor data is saved to: <code>output_path/policies_graph/descriptions_generator.pickle</code></p> <p>2.4. Events Generation The event generation process occurs in three stages:</p> <p>2.4.1. Symbolic Representation Generation    - Converts policies into symbolic format    - Processes in parallel using multiple workers (configured in config)</p> <p>2.4.2. Symbolic Constraints Generation    - Creates constraints based on the symbolic representation    - Uses same worker and timeout configuration as symbolic generation</p> <p>2.4.3. Event Graph Generation    - Final event creation (most time-intensive phase)    - Includes restriction filtering, validation, and result compilation    - Controlled by configurable difficulty levels    - Generates samples in batches according to dataset configuration</p> <p>All generated events are saved to: <code>output_path/datasets/dataset__[timestamp].pickle</code></p> <p>Note: Event generation is cost-controlled via the config settings.</p>"},{"location":"examples/airline/#step-3-analyze-simulator-results","title":"Step 3 - Analyze Simulator Results","text":"<p>After the simulation completes, you can find the results in the specified output path directory (<code>examples/airline/output/run_0</code>). The structure will look like this:</p> <pre><code>experiments/\n\u251c\u2500\u2500 dataset__[timestamp]__exp_[n]/    # Experiment run folder\n\u2502   \u251c\u2500\u2500 experiment.log                # Detailed experiment execution logs\n\u2502   \u251c\u2500\u2500 config.yaml                   # Configuration used for this run\n\u2502   \u251c\u2500\u2500 prompt.txt                    # Prompt template used\n\u2502   \u251c\u2500\u2500 memory.db                     # Dialog memory database\n\u2502   \u2514\u2500\u2500 results.csv                   # Evaluation results and metrics\n\u2502\ndatasets/\n\u251c\u2500\u2500 dataset__[timestamp].pickle       # Generated dataset snapshot\n\u2514\u2500\u2500 dataset.log                       # Dataset generation logs\n\u2502\npolicies_graph/\n\u251c\u2500\u2500 graph.log                         # Policy graph generation logs\n\u2514\u2500\u2500 descriptions_generator.pickle     # Generated descriptions and policies\n</code></pre>"},{"location":"examples/airline/#output-files-overview","title":"Output Files Overview","text":"<ul> <li>experiment.log: Contains detailed logs of the experiment execution, including timestamps and any errors encountered during the run.</li> <li>config.yaml: This file holds the configuration settings that were used for this specific simulation run, allowing for easy replication of results.</li> <li>prompt.txt: The prompt template that was utilized during the simulation, which can be useful for understanding the context of the agent's responses.</li> <li>memory.db: A database file that stores the dialog memory, which can be analyzed to understand how the agent retained and utilized information throughout the simulation.</li> <li>results.csv: This file includes the evaluation results and metrics from the simulation, providing insights into the performance of the agent.</li> </ul> <p>In addition to the experiment folder, you will find: - dataset__[timestamp].pickle: A snapshot of the generated dataset at the time of the simulation, which can be used for further analysis. - dataset.log: Logs related to the dataset generation process, detailing any issues or important events that occurred during this phase. - graph.log: Logs related to the generation of the policy graph, which can help in understanding the generated policies and their relations for the scenarios generation process. - descriptions_generator.pickle: A file containing the generated descriptions and policies, useful for reviewing the agent's learned behaviors and strategies.</p>"},{"location":"examples/airline/#step-4-run-the-simulator-visualization","title":"Step 4 - Run the Simulator Visualization","text":"<p>To visualize the simulation results using streamlit, run:</p> <pre><code>cd simulator/visualization \nstreamlit run Simulator_Visualizer.py\n</code></pre> <p>This will launch a Streamlit dashboard showing detailed analytics and visualizations of your simulation results. In the visualization you can: - Load simulator memory and experiments by providing their full path - View conversation flows and policy compliance - Analyze agent performance and faliure points</p> <p>Note: Make sure you have streamlit installed (<code>pip install streamlit</code>) before running the visualization.</p>"},{"location":"quick_start/Running_the_Simulator/","title":"Running the Simulator","text":"<p>This guide details the steps required to run the simulator.</p>"},{"location":"quick_start/Running_the_Simulator/#1-run-the-simulator","title":"1. Run the Simulator","text":"<p>Use the following command to run the simulator:</p> <pre><code>python run.py --output_path &lt;output_path&gt; [--config_path &lt;config_path&gt;] [--dataset &lt;dataset&gt;]\n</code></pre> <p>Arguments: - <code>--output_path</code>: (Required) Path for saving output files - <code>--config_path</code>: (Optional) Path to config file (default: <code>config/config_default.yml</code>) - <code>--dataset</code>: (Optional) Dataset name (default: <code>'latest'</code>)</p> <p>Example:</p> <pre><code>python run.py --output_path ../output/exp1 --config_path ./config/config_airline.yml\n</code></pre>"},{"location":"quick_start/Running_the_Simulator/#6-view-results","title":"6. View Results","text":"<p>After simulation completion, results will be organized in the following structure:</p> <pre><code>experiments/\n\u251c\u2500\u2500 dataset__[timestamp]__exp_[n]/    # Experiment run folder\n\u2502   \u251c\u2500\u2500 experiment.log                # Detailed execution logs\n\u2502   \u251c\u2500\u2500 config.yaml                   # Configuration used\n\u2502   \u251c\u2500\u2500 prompt.txt                    # Prompt template\n\u2502   \u251c\u2500\u2500 memory.db                     # Dialog memory database\n\u2502   \u2514\u2500\u2500 results.csv                   # Evaluation results\n\ndatasets/\n\u251c\u2500\u2500 dataset__[timestamp].pickle       # Dataset snapshot\n\u2514\u2500\u2500 dataset.log                       # Generation logs\n\npolicies_graph/\n\u251c\u2500\u2500 graph.log                         # Policy graph logs\n\u2514\u2500\u2500 descriptions_generator.pickle     # Generated descriptions\n</code></pre> <p>To visualize the simulation results using streamlit, run:</p> <pre><code>cd simulator/visualization \nstreamlit run Simulator_Visualizer.py\n</code></pre> <p>This will launch a Streamlit dashboard showing detailed analytics and visualizations of your simulation results.</p>"},{"location":"quick_start/configuration/","title":"Configuration","text":"<p>This guide outlines the required setup steps to configure the simulator before running it.</p>"},{"location":"quick_start/configuration/#1-configure-llm-api-keys","title":"1. Configure LLM API Keys","text":"<p>Edit the <code>config/llm_env.yml</code> file with your API credentials:</p> <pre><code>openai:\n  OPENAI_API_KEY: \"your-api-key-here\"\n  OPENAI_API_BASE: ''\n  OPENAI_ORGANIZATION: ''\n\nazure:\n  AZURE_OPENAI_API_KEY: \"your-api-key-here\"\n  AZURE_OPENAI_ENDPOINT: \"your-endpoint\"\n  OPENAI_API_VERSION: \"your-api-version\"\n</code></pre>"},{"location":"quick_start/configuration/#2-configure-simulator-parameters","title":"2. Configure Simulator Parameters","text":"<p>Before running the simulator, configure the <code>config/config_default.yml</code> file. Key configuration sections include:</p> <ul> <li><code>environment</code>: Paths for prompts, tools, and data-scheme folders</li> <li><code>description_generator</code>: Settings for flow extraction and policies</li> <li><code>event_generator</code>: LLM and worker configurations</li> <li><code>dialog_manager</code>: Dialog system and LLM settings</li> <li><code>dataset</code>: Parameters for dataset generation</li> </ul> <p>Important configuration points:</p> <ol> <li>Update file paths in the <code>environment</code> section</li> <li>Configure LLM settings (<code>type</code> and <code>name</code>)</li> <li>Adjust worker settings (<code>num_workers</code> and <code>timeout</code>)</li> <li>Set appropriate <code>cost_limit</code> values</li> </ol> <p>Key points to configure:</p> <ul> <li>Update file paths in the <code>environment</code> section to match your project structure:</li> </ul> <pre><code>prompt_path: 'examples/airline/wiki.md'  # Path to your agent's wiki/documentation\ntools_folder: 'examples/airline/tools'   # Path to your agent's tools\ndatabase_folder: 'examples/airline/data' # Path to your data schema\n</code></pre> <ul> <li>Adjust LLM configurations throughout the file:</li> </ul> <pre><code>llm:\n    type: 'openai'  # or 'azure'\n    name: 'gpt-4o'   # or other model names as gpt-4o-mini\n</code></pre> <ul> <li>Configure worker settings based on your system's capabilities. This parameter is highly important in cases of rate-limit responses:</li> </ul> <pre><code>   num_workers: 3    # Number of parallel workers\n   timeout: 10       # Timeout in seconds\n</code></pre> <ul> <li>Set appropriate cost limits to control API usage:</li> </ul> <pre><code>\ncost_limit: 30    # In dollars, for simulator dialog manager\n\n</code></pre>"},{"location":"quick_start/installation/","title":"Installation","text":"<p>This guide provides detailed instructions for setting up your development environment.</p>"},{"location":"quick_start/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10</li> </ul>"},{"location":"quick_start/installation/#step-by-step-installation","title":"Step-by-Step Installation","text":""},{"location":"quick_start/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone git@github.com:plurai-ai/Chat-Agent-Simulator.git\ncd Chat-Agent-Simulator\n</code></pre>"},{"location":"quick_start/installation/#2-install-dependencies","title":"2. Install Dependencies","text":"<p>Choose one of the following methods to install dependencies:</p>"},{"location":"quick_start/installation/#using-conda","title":"Using Conda","text":"<pre><code>conda env create -f environment_dev.yml\nconda activate Chat-Agent-Simulator\n</code></pre>"},{"location":"quick_start/installation/#using-pip","title":"Using pip","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"quick_start/installation/#using-pipenv","title":"Using pipenv","text":"<pre><code>pip install pipenv\npipenv install\n</code></pre>"}]}