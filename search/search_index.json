{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IntellAgent: Uncover Your Agent's Blind Spots","text":"<p>Companies today face a critical challenge when deploying chatbot agents: balancing capability with reliability. Most organizations end up severely limiting their chatbots' capabilities (restricting them to read-only operations or minimal tool access) due to concerns about:</p> <ul> <li>Policy violations</li> <li>Inconsistent responses</li> <li>Unexpected edge cases</li> </ul> <p>This is where IntellAgent transforms the landscape. Instead of restricting your agent's capabilities due to uncertainty, IntellAgent empowers you to:</p> <ol> <li>\ud83d\udd0d Discover the Unknown: Generate thousands of edge-case scenarios that stress-test your agent from every angle</li> <li>\u26a0\ufe0f Identify Failure Points: Proactively detect and understand where your agent might fail</li> <li>\ud83d\udcc8 Optimize Capabilities: Find the optimal balance between functionality and reliability</li> <li>\u2705 Deploy with Confidence: Release chat agents that are both powerful AND reliable</li> </ol> <p>IntellAgent turns the traditional \"restrict-by-default\" approach into a data-driven \"enable-with-confidence\" strategy. By thoroughly simulating and analyzing your agent's behavior, you can expand its capabilities while maintaining strict reliability standards.</p> <p>Don't limit your chatbot's potential because of what you don't know. Use IntellAgent to know exactly what your agent can handle, fix what it can't, and deploy with confidence.</p> <ul> <li> <p> Get Started</p> <p>Get started quickly using these tutorials.</p> <p> Get Started</p> </li> <li> <p> How IntellAgent works</p> <p>In-depth explanations on IntellAgent system and the main concepts</p> <p> How IntellAgent works</p> </li> <li> <p> Customization &amp; Integrations</p> <p>How to customize easily IntellAgent to your chatbot environment</p> <p> How-to Guides</p> </li> <li> <p> Examples</p> <p>In-depth example how to configure the system</p> <p> Examples</p> </li> </ul>"},{"location":"How_it_Works/architecture/","title":"Architecture System Overview","text":"<p>The IntellAgent system pipeline consists of three main components:</p> <ol> <li> <p>Event Generation</p> <ul> <li>Input: Database schema and either:<ul> <li>Chatbot system prompt, or</li> <li>Company policy documentation</li> </ul> </li> <li>Output:<ul> <li>Policy graph representation</li> <li>Generated events with varying complexity levels, including:<ul> <li>Scenario descriptions</li> <li>User requests</li> <li>Initial database states</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Dialog Simulation</p> <ul> <li>Input: Generated events and configured agents</li> <li>Output: <ul> <li>Complete conversation transcripts</li> <li>Tool usage logs</li> <li>Agent reasoning traces</li> </ul> </li> </ul> </li> <li> <p>Fine-Grained Analysis</p> <ul> <li>Input: Dialog transcripts and policy requirements</li> <li>Output:<ul> <li>Detailed performance metrics</li> <li>Policy compliance analysis</li> <li>Complexity-based evaluation reports</li> </ul> </li> </ul> </li> </ol> <p>All the agents in IntellAgent are built using LagGraph framework </p>"},{"location":"How_it_Works/architecture/#1-event-generation","title":"1. Event Generation","text":"<p>The IntellAgent Event Generator creates realistic chatbot interactions through a multi-stage pipeline that transforms high-level policies into concrete scenarios with corresponding database states. The process consists of three main components:</p>"},{"location":"How_it_Works/architecture/#11-policy-analysis-graph-construction","title":"1.1 Policy Analysis &amp; Graph Construction","text":"<p>The <code>DescriptionGenerator</code> class handles the initial policy analysis through these steps:</p> <ol> <li>Flow Extraction: Breaks down the system prompt into distinct flows</li> <li>Policy Extraction: Analyzes each flow to identify individual policies and assigns complexity scores</li> <li>Graph Construction: Creates a weighted graph where:<ul> <li>Nodes: Individual policies with complexity scores</li> <li>Edges: Weighted connections (1-10) indicating the likelihood of two policies appearing together in the same task</li> <li>Edge weights are determined through LLM evaluation of policy pairs</li> </ul> </li> </ol>"},{"location":"How_it_Works/architecture/#12-event-generation-pipeline","title":"1.2 Event Generation Pipeline","text":""},{"location":"How_it_Works/architecture/#121-description-generation","title":"1.2.1 Description Generation","text":"<p>The system generates event descriptions through:</p> <ol> <li>Sampling policies based on target complexity:<ul> <li>Starts with a random policy node</li> <li>Performs weighted random walks based on edge weights</li> <li>Continues until reaching desired complexity threshold</li> </ul> </li> <li>Converting selected policies into natural language scenarios</li> <li>Generating expected chatbot behaviors</li> <li>(Optional) Refining expected behaviors through feedback iterations</li> </ol>"},{"location":"How_it_Works/architecture/#122-symbolic-representation","title":"1.2.2 Symbolic Representation","text":"<p>The <code>EventsGenerator</code> class transforms descriptions into concrete events by:</p> <ol> <li>Creating symbolic representations of entities and relationships</li> <li>Defining database constraints based on policies</li> <li>Converting symbolic representations into actual database states through:<ul> <li>Table-specific insertion tools</li> <li>Validation of referential integrity</li> <li>Parallel processing of multiple events</li> </ul> </li> </ol> <p></p>"},{"location":"How_it_Works/architecture/#2dialog-simulation-architecture","title":"2.Dialog Simulation Architecture","text":"<p>The Dialog Simulation system orchestrates conversations between simulated users and chatbots. The system consists of two main components: the Dialog Graph and Dialog Manager.</p>"},{"location":"How_it_Works/architecture/#dialog-graph","title":"Dialog Graph","text":"<p>The Dialog Graph implements a state machine that manages the conversation flow between the simulated user and chatbot. Key features include:</p> <ul> <li> <p>State Management: Uses <code>DialogState</code> to track:</p> <ul> <li>User and chatbot message history</li> <li>User thoughts and reasoning</li> <li>Critique feedback</li> <li>Stop signals for conversation termination</li> </ul> </li> <li> <p>Node Types:</p> <ul> <li>User Node: Simulates user responses and tracks reasoning</li> <li>Chatbot Node: Handles chatbot responses and tool interactions</li> <li>Critique Node: Evaluates conversation adherence to policies</li> </ul> </li> <li> <p>Flow Control:</p> <ul> <li>Manages message passing between participants</li> <li>Handles conversation termination conditions</li> <li>Supports conversation feedback loops through critique evaluation</li> </ul> </li> </ul>"},{"location":"How_it_Works/architecture/#dialog-manager","title":"Dialog Manager","text":"<p>The Dialog Manager controls the execution of dialogs and provides the infrastructure for running simulations. Key features include:</p> <ul> <li> <p>Configuration Management:</p> <ul> <li>Configures LLM models for user, chatbot, and critique agents</li> <li>Sets up environment tools and schemas</li> <li>Manages prompt templates and parsing functions</li> </ul> </li> <li> <p>Execution Modes:</p> <ul> <li>Synchronous execution (<code>run</code>)</li> <li>Asynchronous execution (<code>arun</code>)</li> <li>Batch processing of multiple events (<code>run_events</code>)</li> </ul> </li> <li> <p>Memory Management:</p> <ul> <li>SQLite-based conversation storage</li> <li>Tracks tool calls and their outputs</li> <li>Maintains conversation history</li> </ul> </li> </ul>"},{"location":"How_it_Works/architecture/#simulation-flow","title":"Simulation Flow","text":"<ol> <li>The Dialog Manager initializes the environment and agents</li> <li>For each event:<ul> <li>User agent receives event details and expected behaviors</li> <li>Conversation alternates between user and chatbot</li> <li>Critique agent evaluates responses against policies</li> <li>Conversation continues until success or policy violation</li> </ul> </li> </ol>"},{"location":"How_it_Works/architecture/#key-features","title":"Key Features","text":"<ul> <li>Policy Enforcement: Built-in critique system to evaluate chatbot adherence to defined policies</li> <li>Parallel Processing: Support for parallel execution of multiple conversations</li> <li>Extensible Architecture: Modular design allowing for custom LLMs, tools, and evaluation criteria</li> <li>Comprehensive Logging: Detailed tracking of conversations, tool usage, and agent reasoning</li> </ul>"},{"location":"How_it_Works/architecture/#3-fine-grained-analysis","title":"3. Fine-Grained Analysis","text":"<p>The Dialog critique component performs a detailed evaluation of the conversation by analyzing:</p> <ol> <li>The user-chatbot dialog history</li> <li>The chatbot's system prompt</li> <li>The termination reason provided by the user agent</li> </ol> <p>The critique process follows these steps:</p> <ol> <li> <p>Termination Validation: Verifies if the stated reason for dialog termination is accurate</p> <ul> <li>If incorrect: Provides feedback through <code>critique_feedback</code> and continues the dialog</li> <li>If correct: Proceeds with policy analysis</li> </ul> </li> <li> <p>Multi-Level Policy Coverage Analysis: </p> <ul> <li>Identifies which event policies were tested during the conversation</li> <li>Determines which policies were violated (if any)</li> <li>Evaluates policies across different complexity levels, ranging from straightforward tasks to highly complex and nuanced edge-case scenarios  </li> </ul> </li> <li>Report Generation: Creates a detailed performance assessment based on the above analysis, with insights categorized by policies and complexity level</li> </ol>"},{"location":"How_it_Works/how-it-works/","title":"How Chat-Agent-Simulator works","text":"<p>IntellAgent is a cutting-edge multi-agent framework designed to provide fine-grained diagnostics for chatbot systems. It simulates thousands of edge-case scenarios to thoroughly assess chatbot performance. This document provides a detailed overview of the step-by-step simulation process.</p>"},{"location":"How_it_Works/how-it-works/#pipeline-overview","title":"Pipeline Overview","text":""},{"location":"How_it_Works/how-it-works/#1-input-context-analysis","title":"1. Input Context Analysis","text":"<p>The framework begins by analyzing the agent's core inputs: - Prompt: Chat-agent system prompt or a document outlining the company policies - Tools: Chat-agent tools  - Database Schema: Underlying data structure - Database Validators: (Optional) Custom validation functions - Task Description: (Optional) Automatically inferred from system prompt</p> <p>This analysis forms the baseline for understanding the agent's capabilities and constraints.</p> <p></p>"},{"location":"How_it_Works/how-it-works/#2-flow-and-policies-generation","title":"2. Flow and Policies Generation","text":"<p>The system automatically: - Decomposes the agent's primary task into several main flows - For each flow, extracts relevant policies, category and challenge score </p> Flow Policy Example Category Challenge Score Book Flight \"Each reservation can use at most one travel certificate, one credit card, and three gift cards.\" Payment Handling / Financial 3 Book Flight \"Flights booked within 7 days of departure are non-refundable unless a travel insurance policy is purchased.\" Policy Enforcement / Restriction 4 Modify Flights \"Basic economy flights cannot be modified. Other reservations can be modified without changing the origin or trip.\" Policy Enforcement / Restriction 4 Modify Flights \"Modifications made less than 24 hours before departure will incur a $75 fee per passenger.\" Fee Policy / Upsell Handling 3 Cancel Flight \"All reservations can be cancelled within 24 hours of booking or if the airline cancelled the flight.\" Policy Enforcement / Restriction 3 Cancel Flight \"Cancellations made after 24 hours will result in a travel credit valid for one year.\" Refund Handling / Credit Policy 3 Refund \"Refunds will go to original payment methods within 5 to 7 business days.\" Payment Handling / Financial 2 Refund \"Refunds for non-refundable tickets are available only if the flight is cancelled by the airline.\" Policy Enforcement / Restriction 4"},{"location":"How_it_Works/how-it-works/#3-policies-graph-generation","title":"3. Policies Graph Generation","text":"<p>IntellAgent creates a weighted graph structure where: - Nodes represent individual policies - Each node is assigned a weight that reflects the complexity of its associated policy  - Edge weights reflect (on a scale of 1\u201310) the likelihood of the two policies co-occurring in a conversation</p> <p>This graph facilitates creation of natural user requests that cover various policies at various levels of complexity.</p> <p></p>"},{"location":"How_it_Works/how-it-works/#4-dataset-event-generation","title":"4. Dataset Event Generation","text":"<p>The system generates comprehensive test scenarios including: - Scenario Content: Specific user interaction cases - Expected Behavior: Desired agent behavior based on policies - Data Ingestion: Required database records and state prerequisites</p> <p>Each scenario is assigned varying challenge scores to ensure thorough testing across complexity levels.</p> <p></p>"},{"location":"How_it_Works/how-it-works/#5-dialog-simulation","title":"5. Dialog Simulation","text":"<p>The dialog simulation process involves a sophisticated interaction between multiple components to thoroughly test the chatbot's capabilities:</p>"},{"location":"How_it_Works/how-it-works/#51-user-chatbot-interaction","title":"5.1 User-Chatbot Interaction","text":"<p>For each event in the database, IntellAgent orchestrates a dynamic conversation between: - A User Agent that simulates user behavior - The Target Chatbot being evaluated - A Dialog Critique component that monitors the interaction</p> <p>The simulation follows these steps:</p> <ol> <li>The User Agent receives:<ul> <li>Event description and context</li> <li>Database state information</li> <li>Expected chatbot behaviors based on policy requirements</li> </ul> </li> <li>The interaction proceeds step-by-step with the User Agent making decisions based on the chatbot's responses</li> <li>The User Agent can terminate the conversation when either:<ul> <li>The task is successfully completed</li> <li>The chatbot violates a policy or deviates from expected behavior</li> </ul> </li> </ol> <p></p>"},{"location":"How_it_Works/how-it-works/#6-dialog-critique-system-comprehensive-evaluation","title":"6. Dialog Critique System &amp; Comprehensive Evaluation","text":"<p>The critique component performs real-time monitoring and post-conversation analysis:</p> <ol> <li> <p>Real-Time Dialog Monitoring</p> <ul> <li>Tracks conversation flow and policy compliance in real-time</li> <li>Validates each interaction against defined policies</li> <li>Flags potential issues for immediate intervention</li> </ul> </li> <li> <p>Policy Analysis</p> <ul> <li>Identifies which event policies were actively tested during the conversation</li> <li>Documents any policy violations or deviations from expected behavior</li> <li>Generates a detailed compliance report</li> </ul> </li> <li> <p>Performance Metrics</p> <ul> <li>Calculates quantitative metrics (success rate, completion time, policy coverage)</li> <li>Evaluates qualitative aspects (conversation flow, user satisfaction)</li> <li>Identifies patterns in successful vs. failed interactions</li> <li>Provides actionable recommendations for improvement</li> </ul> </li> </ol> <p>The system generates two primary types of analysis:</p> <ul> <li>Category-based Analysis: Success rates and common issues across different policy types</li> <li>Complexity-based Analysis: Performance metrics across varying challenge levels</li> </ul> Success Rates Across Policy Categories Success Rates Across Challenge Levels"},{"location":"contribution/contributing/","title":"Contributing to IntellAgent","text":"<p>Thank you for considering contributing to Chat-Agent-Simualtor! We deeply appreciate your interest in improving our project. We are still in the early stages of development and welcome contributions from the community. We are particularly interested in receiving new examples and environments to enrich the project. If you wish to be a part of our journey, we invite you to connect with us through our Discord Community. We're excited to have you onboard!</p>"},{"location":"contribution/contributing/#bug-fixes-and-documentation-enhancements","title":"Bug Fixes and Documentation Enhancements","text":"<p>Bug fixes and documentation improvements, including compelling examples and use cases, greatly benefit our project. If you encounter any bugs or identify areas where the documentation could be strengthened, please do not hesitate to submit a pull request (PR) containing your proposed changes.</p>"},{"location":"contribution/contributing/#feature-requests","title":"Feature Requests","text":"<p>For significant feature additions, we encourage you to open an issue on GitHub. Additionally, we invite you to join our Discord community and engage in discussions about the feature in the #features-requests channel. This collaborative environment enables us to delve deeper into the proposed features and foster meaningful dialogue.</p> <p>We value your contributions and look forward to working together to enhance Chat-Agent-Simualtor!</p>"},{"location":"customization/checkpoints/","title":"Checkpoints and Cost Saving and Monitoring","text":"<p>The IntellAgent system is optimized for cost efficiency and provides multiple levels of checkpoints to reduce expenses.</p> <p>This document outlines the different types of checkpoints and their locations within the pipeline.</p>"},{"location":"customization/checkpoints/#policies-graph-checkpoint","title":"Policies Graph Checkpoint","text":"<p>When initializing the <code>SimulatorExecutor</code>, the system searches for the <code>DescriptionGenerator</code> checkpoint at:  </p> <pre><code>&lt;args.output_path&gt;/policies_graph/descriptions_generator.pickle\n</code></pre> <p>If the checkpoint exists, the <code>descriptions_generator</code> (which contains the policies graph) will be loaded. If it does not exist, the system will generate the graph. During execution, the system automatically saves the <code>descriptions_generator</code> to this path.</p> <p>As a result, it will be loaded by default in subsequent runs unless the <code>output_path</code> is changed.</p>"},{"location":"customization/checkpoints/#dataset-events-checkpoints","title":"Dataset Events Checkpoints","text":"<p>Generating dataset events is a computationally expensive process. It is recommended to run it once and reuse the same dataset for all experiments to ensure consistency across experiments. You can specify the dataset name using the <code>--dataset</code> argument. The dataset will be saved at the following location:</p> <pre><code>&lt;args.output_path&gt;/datasets/&lt;dataset_name&gt;.pickle\n</code></pre> <p>During each iteration of size <code>mini_batch_size</code> (as defined in the configuration file), all generated events up to that point will be saved as checkpoints. This allows you to resume the dataset generation from the last mini-batch if it is interrupted.</p> <p>By default, the <code>--dataset</code> argument is set to <code>latest</code>, which will automatically load the most recently generated dataset.</p> <p>Additionally, you can set a <code>cost_limit</code> (in dollars) by defining the <code>cost_limit</code> variable in the configuration file. Note that this feature may not be supported by all models.</p>"},{"location":"customization/checkpoints/#experiment-checkpoint","title":"Experiment Checkpoint","text":"<p>Running the simulator on all events can be both costly and time-consuming. It is important to note that, unlike previous checkpoints, a new experiment is generated by default at each run.</p> <p>The experiment name can be specified using the <code>--experiment</code> variable. If not provided, it will default to <code>exp_{i}</code>, where <code>i</code> is a sequential number. All experiment dumps will be saved in the following folder:</p> <pre><code>&lt;args.output_path&gt;/experiments/&lt;dataset_name&gt;__&lt;experiment_name&gt;\n</code></pre> <p>The simulator results are saved after every <code>mini_batch_size</code> (as defined in the configuration file). If the run is interrupted and you want to resume it, you need to set the <code>--experiment</code> variable to the <code>experiment_name</code>.</p> <p>Additionally, you can define a <code>cost_limit</code> (in dollars) in the configuration file by setting the <code>cost_limit</code> variable. Note that this feature may not be supported by all models.</p>"},{"location":"customization/custom_chatbot/","title":"Custom Chatbot Setup","text":"<p>This guide explains how to configure the chatbot agent in the simulator. Currently, two types of customizations are supported:</p> <ol> <li>Modifying the LLM in the default tool-calling chatbot agent.</li> <li>Providing your own LangGraph-based chatbot agent.</li> </ol> <p>In the future, additional integrations will be available for platforms like crewAI and AutoGEN.</p> <p>If you additional requirements or need us to handle the full integration, we offer this service. Feel free to reach out to us via  Plurai.ai for more information.</p>"},{"location":"customization/custom_chatbot/#prerequisites","title":"Prerequisites:","text":"<p>Before customizing the chatbot, ensure the environment is set up by following the instructions here.</p>"},{"location":"customization/custom_chatbot/#tool-calling-agent","title":"Tool-Calling Agent","text":"<p>If you're using a basic tool-calling agent, the only required modification (apart from environment customization) is selecting the appropriate LLM. IntellAgent supports all LangChain-compatible tool supported LLMs. Simply define the LLM type and name in the configuration file according to your chatbot settings:  </p> <pre><code>llm_chat:\n    type: ''\n    name: ''\n</code></pre>"},{"location":"customization/custom_chatbot/#setting-the-system-prompt","title":"Setting the System Prompt","text":"<p>By default, the agent uses the system prompt specified in the environment settings, along with the welcome message: \"Hello! \ud83d\udc4b I'm here to help with any request you might have.\" </p> <p>To customize the system prompt or the welcome message, you need to define <code>dialog_manager.chatbot_initial_messages</code> before running the simulator. The variable should be a <code>List[BaseMessage]</code> (a list of LangChain base messages).  </p> <p>Here\u2019s an example of how to modify the <code>run.py</code> main function to use a custom prompt:  </p> <pre><code>from langchain_core.messages import AIMessage, SystemMessage\n\n# Initialize the simulator executor with the environment configuration\nexecutor = SimulatorExecutor(config, args.output_path)\n\n# Load the dataset (default is the latest). To load a specific dataset, pass its path.\nexecutor.load_dataset(args.dataset)\n\n# Define the initial messages (system prompt and welcome message)\nmessages = [\n    SystemMessage(content='Enter the chatbot system message here'),\n    AIMessage(content=\"Hello, how can I help you?\")\n]\n\n# Update the default initial messages\nexecutor.dialog_manager.chatbot_initial_messages = messages\n\n# Run the simulation on the dataset\nexecutor.run_simulation()\n</code></pre>"},{"location":"customization/custom_chatbot/#general-langgraph-agent","title":"General LangGraph Agent","text":"<p>If your chatbot is based on LangGraph, you can customize the chatbot agent to fit your graph architecture. Ensure that your graph adheres to the following requirements:</p> <ul> <li> <p>The graph state must include the following (additional variables should be optional): <code>python messages: Annotated[list[AnyMessage]] args: dict[Any]</code></p> </li> <li> <p>The <code>messages</code> list should contain all interactions between the user and the chatbot. If internal calls made by the chatbot need to be tracked (e.g., it was part of the provided policies), this can be done using tool-calling messages:</p> </li> </ul> <pre><code>from langchain_core.messages import AIMessage, ToolMessage\n\nmessages += [\n    AIMessage(\n        content=\"\",\n        tool_calls=[\n            {'name': 'get_product_details', 'args': {'product_id': 'P2'}, 'id': '1', 'type': 'tool_call'}\n        ]\n    ),\n    ToolMessage(content='The product is not available currently in the store')\n]\n ```\n\n- If certain tools require database access, the database will be in the graph state within the `args` variable under the `data` key.\nIf the tool is properly defined (see [custom tools](./custom_environment.md#tools_file) with the `data` variable decorated by `InjectedState`, the agent should include the variable before invoking the tool. Below is an example of how this is handled in the base tool graph implementation: [tool graph](../simulator/agents_graphs/langgraph_tool.py):\n````python\nfor tool_call in state[\"messages\"][-1].tool_calls:\n    tool = self.tools_by_name[tool_call[\"name\"]]\n    all_tool_args = list(inspect.signature(tool.func).parameters)\n    function_args = copy.deepcopy(tool_call[\"args\"])\n    if state['args'] is not None:\n        function_args.update({k: v for k, v in state['args'].items()\n                              if (k in all_tool_args) and (k not in function_args)})\n        observation = tool.func(**function_args)\n ````\n\nIf you have a LangGraph compiled graph that satisfies these conditions, set it in `dialog_manager.chatbot` before running the simulator. Here's an example of how to modify the `run.py` main function to use a custom graph:\n\n```python\n# Initialize the simulator executor with the environment configuration\nexecutor = SimulatorExecutor(config, args.output_path)\n\n# Load the dataset (default is the latest). To load a specific dataset, pass its path.\nexecutor.load_dataset(args.dataset)\n\n# Set the chatbot graph\nexecutor.dialog_manager.chatbot = chatbot\n\n# Run the simulation on the dataset\nexecutor.run_simulation()\n</code></pre> <p>To provide a system prompt to the agent, configure the <code>initial_messages</code> variable as described here.</p>"},{"location":"customization/custom_environment/","title":"Custom Chat-Agent Environment Setup","text":"<p>This guide provides instructions for setting up a custom environment. We also provide two examples of custom environments: Education (basic) and Airline (advanced).</p> <p>If you have a complex environment and need assistance with the integration, we offer this service. Feel free to reach out to us via  Plurai.ai for more information.</p> <p>You should create a new config_env.yml file and define there the chatbot environment variables</p> <pre><code>environment:\n    prompt_path:  # Path to prompt\n    tools_file: # Optional! Path to a python script that include all the tools functions \n    database_folder: #  Optional! Path to database folder\n    database_validators: # Optional! Path to the file with the validators functions\n</code></pre> <p>After defining properly all the variables, you can run the simulator on the new custom environment:</p> <pre><code>python run.py \\\n    --output_path PATH     # Required: Directory where output files will be saved\n    --config_path PATH     # Optional: Path to config_env.yml (default: ./config_default.yml)\n    --dataset NAME         # Optional: Dataset name to use (default: 'latest')\n</code></pre>"},{"location":"customization/custom_environment/#environment-variables","title":"Environment Variables","text":""},{"location":"customization/custom_environment/#prompt_path","title":"prompt_path","text":"<p>This variable specifies the path to a prompt file. The file should be a simple text file (<code>.txt</code>) or a markdown file (<code>.md</code>) containing the desired prompt. The prompt doesn't have to be the exact chatbot system prompt, it can also be a document that describes the list of policies that should be tested. In this case the chatbot system prompt should be also provided (see tool chatbot modification).</p> <p>Example of prompt file: For a complete example of a prompt file, see the airline chat-agent system prompt.</p>"},{"location":"customization/custom_environment/#database_folder","title":"database_folder","text":"<p>This variable specifies the path to a folder containing CSV files. Each CSV file represents a database table used by the system and must include at least one row as an example. It is recommended to provide meaningful and indicative names for the columns in each CSV file.</p> <p>Example of database_folder: For a complete example of a database folder, see the airline chat-agent database scheme folder.</p> <p>The folder should contain CSV files that define your database tables. Here's an example structure from an airline booking system:</p> <p>flights.csv</p> flight_number origin destination scheduled_departure_time_est scheduled_arrival_time_est dates HAT001 PHL LGA 06:00:00 07:00:00 {\"2024-05-16\": {\"status\": \"available\", \"available_seats\": {\"basic_economy\": 16, \"economy\": 10, \"business\": 13}, \"prices\": {\"basic_economy\": 87, \"economy\": 122, \"business\": 471}}} <p>reservations.csv</p> reservation_id user_id origin destination flight_type cabin flights passengers payment_history created_at total_baggages nonfree_baggages insurance 4WQ150 chen_jackson_3290 DFW LAX round_trip business [{\"origin\": \"DFW\", \"destination\": \"LAX\", \"flight_number\": \"HAT170\", \"date\": \"2024-05-22\"}] [{\"first_name\": \"Chen\", \"last_name\": \"Jackson\", \"dob\": \"1956-07-07\"}] [{\"payment_id\": \"gift_card_3576581\", \"amount\": 4986}] 2024-05-02 03:10:19 5 0 no <p>users.csv</p> user_id name address email dob payment_methods saved_passengers membership reservations mia_li_3668 {\"first_name\": \"Mia\", \"last_name\": \"Li\"} {\"address1\": \"975 Sunset Drive\", \"city\": \"Austin\", \"country\": \"USA\"} mia.li@example.com 1990-04-05 {\"credit_card_4421486\": {\"source\": \"credit_card\", \"last_four\": \"7447\"}} [] gold [\"NO6JO3\"]"},{"location":"customization/custom_environment/#tools_file-optional","title":"tools_file (optional)","text":"<p>This variable specifies the path to a python script containing all the agent tool functions. </p> <p>The tool functions must be implemented using one of the following approaches: - Using LangChain's <code>@tool</code> decorator: LangChain Tool Decorator Guide - Using LangChain's <code>StructuredTool</code>: LangChain StructuredTool Guide</p> <p>If the tool needs to access the database you should add to the function a variable 'data', and use langchain InjectedState class.  In the following way:</p> <pre><code>def tool_function(data: Annotated[dict, InjectedState(\"dataset\")]):\n</code></pre> <p>The data variable will contain a dictionary of dataframe, where the name is the table name (according to the csv file name in the database folder).</p> <p>Optionally, you can define a tool schema by creating a variable named <code>&lt;function_name&gt;_schema</code>. If no schema variable is provided, the system will infer the schema automatically.</p> <p>Example of a valid <code>tools_file</code>: See airline chat-agent tools python script for reference.</p>"},{"location":"customization/custom_environment/#database_validators-optional","title":"database_validators (optional)","text":"<p>Data validators are crucial components of the system. These functions guide the database generation pipeline, ensuring data integrity and consistency. They are particularly important when dealing with duplicate information across different tables, as they allow for consistency checks.</p> <p>The <code>database_validators</code> variable specifies the path to a Python script that contains the data validation functions.</p> <p>To define a validation function, use the <code>@validator</code> decorator and specify the table to which the function applies.</p> <p>Example Validator Function:</p> <pre><code>from simulator.utils.file_reading import validator\n\n@validator(table='users')\ndef user_id_validator(new_df, dataset):\n    if 'users' not in dataset:\n        return new_df, dataset\n    users_dataset = dataset['users']\n    for index, row in new_df.iterrows():\n        if row['user_id'] in users_dataset.values:\n            error_message = f\"User id {row['user_id']} already exists in the users data. You should choose a different user id.\"\n            raise ValueError(error_message)\n    return new_df, dataset\n</code></pre> <ul> <li>The <code>@validator</code> decorator requires the table name as an argument.</li> <li>The validator function is applied before new data is inserted into the database.</li> </ul> <p>For a complete example of validators in action, see the airline booking system validators at airline chat-agent database validators python script. This example includes validators for: - User ID validation (preventing duplicate users) - Flight ID validation (ensuring unique flight numbers) - Flight validation (verifying flight details in reservations) - User validation (maintaining consistency between reservations and user data)</p>"},{"location":"examples/airline/","title":"Step-by-Step Guide to Running the Airline Agent","text":"<p>The airline chatbot offers an advance example of a chatbot structure that operates with tools and database. For more simple example, refer to the education example.</p> <p>The environment (prompt, tools and db schema) was adapted from tau-benchmark, and modified to integrate with the IntellAgent framework. </p>"},{"location":"examples/airline/#step-1-understand-the-input","title":"Step 1 - Understand the input","text":"<p>The <code>examples/airline/input</code> folder contains the following structure:</p> <pre><code>airline/\n\u2514\u2500\u2500 input/\n    \u251c\u2500\u2500 wiki.md                       # Prompt for the airline agent\n    \u251c\u2500\u2500 tools/                        # Directory containing all the agent tools \n    \u2502   \u251c\u2500\u2500 agent_tools.py            # Python script containing all the agent tools that are part of the enviorment\n    \u2502   \u251c\u2500\u2500 book_reservation_tool.py  # Python script containing the book reservation tool\n    \u2502   \u251c\u2500\u2500 ..  # All the rest of the tools\n    \u2502   \u2514\u2500\u2500 util.py  # utils functions that are used by the tools\n    \u251c\u2500\u2500 data/                  # Directory containing data schema (csv with one example)\n    \u2502   \u251c\u2500\u2500 flights.csv            # Flights data scheme and example \n    \u2502   \u251c\u2500\u2500 reservation.csv          # Reservation data scheme and example\n    \u2502   \u2514\u2500\u2500 users.csv                # Users data scheme and example\n    \u2514\u2500\u2500 validatiors/           # Directory containing the data validators\n         \u2514\u2500\u2500 data_validators.py   # The datavalidators\n\n</code></pre> <p>The folders contain all the essential inputs for the environment, including the prompt, database schema, tools, and data validators. For more details about the environment inputs, refer to customization.</p>"},{"location":"examples/airline/#step-2-configure-the-simulator-run-parameters","title":"Step 2 - Configure the Simulator Run Parameters","text":"<p>The parameters in the configuration files have already been updated as follows in the config/config_airline.yml file:</p> <pre><code>environment:\n    prompt_path: 'examples/input/airline/wiki.md'  # Path to your agent's wiki/documentation\n    tools_file: 'examples/input/airline/tools/agent_tools.py'   # Path to your agent's tools \n    database_folder: 'examples/input/airline/data' # Path to your data schema\n    database_validators: 'examples/airline/input/validators/data_validators.py' # Optional! Path to the file with the validators functions\n</code></pre> <p>If you also want to modify the LLMs used by either the simulator or the chatbot, refer to customization.</p>"},{"location":"examples/airline/#step-3-run-the-simulator-and-understand-the-output","title":"Step 3 - Run the Simulator and Understand the Output","text":""},{"location":"examples/airline/#running-the-simulation","title":"Running the Simulation","text":"<p>Execute the simulator using:</p> <pre><code>python run.py --config ./config/config_airline.yml --output_path results/airline\n</code></pre>"},{"location":"examples/airline/#understanding-the-descriptor-generator-output","title":"Understanding the Descriptor Generator Output","text":"<p>The simulator processes your input in several stages:</p> <p>2.0. Task Description Generation</p> <ul> <li>Automatically inferred from the prompt (can be manually specified in <code>config_airline.yml</code>)</li> <li>Defines the chatbot's role as an airline agent handling reservations within policy constraints</li> </ul> <p>2.1. Flow Extraction    The system identifies four main flows:</p> <ul> <li>Book flight</li> <li>Modify flight</li> <li>Cancel flight</li> <li>Refund</li> </ul> <p>2.2. Policy Extraction    Each flow has associated policies. Examples:</p> Flow Policy Example Category Challenge Score Book Flight Agent must obtain user ID, trip type, origin, and destination Knowledge extraction 2 Modify Flight All reservations can change cabin without changing flights Company policy 2 Cancel Flight Cancellation allowed within 24h of booking or airline cancellation Logical Reasoning 4 Refund Compensation available for eligible members based on status/insurance Company policy 3 <p>2.3. Relations Graph Generation</p> <ul> <li>Creates a network of policy relationships</li> <li>Nodes: Individual policies</li> <li>Edges: Policy relationships</li> <li>Weights: Combined challenge scores</li> </ul> <p>All descriptor data is saved to: <code>output_path/policies_graph/descriptions_generator.pickle</code></p> <p>2.4. Events Generation The event generation process occurs in three stages:</p> <p>2.4.1. Symbolic Representation Generation</p> <ul> <li>Converts policies into symbolic format</li> <li>Processes in parallel using multiple workers (configured in config)</li> </ul> <p>2.4.2. Symbolic Constraints Generation</p> <ul> <li>Creates constraints based on the symbolic representation</li> <li>Uses same worker and timeout configuration as symbolic generation</li> </ul> <p>2.4.3. Event Graph Generation</p> <ul> <li>Final event creation (most time-intensive phase)</li> <li>Includes restriction filtering, validation, and result compilation</li> <li>Controlled by configurable difficulty levels</li> <li>Generates samples in batches according to dataset configuration</li> </ul> <p>All generated events are saved to: <code>output_path/datasets/dataset__[timestamp].pickle</code></p> <p>Note: Event generation is cost-controlled via the config settings.</p>"},{"location":"examples/airline/#step-4-analyze-simulator-results","title":"Step 4 - Analyze Simulator Results","text":"<p>After the simulation completes, you can find the results in the specified output path directory (<code>results/airline</code>). The structure will look like this:</p> <pre><code>experiments/\n\u251c\u2500\u2500 dataset__[timestamp]__exp_[n]/    # Experiment run folder\n\u2502   \u251c\u2500\u2500 experiment.log                # Detailed experiment execution logs\n\u2502   \u251c\u2500\u2500 config.yaml                   # Configuration used for this run\n\u2502   \u251c\u2500\u2500 prompt.txt                    # Prompt template used\n\u2502   \u251c\u2500\u2500 memory.db                     # Dialog memory database\n\u2502   \u2514\u2500\u2500 results.csv                   # Evaluation results and metrics\n\u2502\ndatasets/\n\u251c\u2500\u2500 dataset__[timestamp].pickle       # Generated dataset snapshot\n\u2514\u2500\u2500 dataset.log                       # Dataset generation logs\n\u2502\npolicies_graph/\n\u251c\u2500\u2500 graph.log                         # Policy graph generation logs\n\u2514\u2500\u2500 descriptions_generator.pickle     # Generated descriptions and policies\n</code></pre>"},{"location":"examples/airline/#output-files-overview","title":"Output Files Overview","text":"<ul> <li>experiment.log: Contains detailed logs of the experiment execution, including timestamps and any errors encountered during the run.</li> <li>config.yaml: This file holds the configuration settings that were used for this specific simulation run, allowing for easy replication of results.</li> <li>prompt.txt: The prompt template that was utilized during the simulation, which can be useful for understanding the context of the agent's responses.</li> <li>memory.db: A database file that stores the dialog memory, which can be analyzed to understand how the agent retained and utilized information throughout the simulation.</li> <li>results.csv: This file includes the evaluation results and metrics from the simulation, providing insights into the performance of the agent.</li> </ul> <p>In addition to the experiment folder, you will find:</p> <ul> <li>dataset__[timestamp].pickle: A snapshot of the generated dataset at the time of the simulation, which can be used for further analysis.</li> <li>dataset.log: Logs related to the dataset generation process, detailing any issues or important events that occurred during this phase.</li> <li>graph.log: Logs related to the generation of the policy graph, which can help in understanding the generated policies and their relations for the scenarios generation process.</li> <li>descriptions_generator.pickle: A file containing the generated descriptions and policies, useful for reviewing the agent's learned behaviors and strategies.</li> </ul>"},{"location":"examples/airline/#step-5-run-the-simulator-visualization","title":"Step 5 - Run the Simulator Visualization","text":"<p>To visualize the simulation results using streamlit, run:</p> <pre><code>cd simulator/visualization \nstreamlit run Simulator_Visualizer.py\n</code></pre> <p>This will launch a Streamlit dashboard showing detailed analytics and visualizations of your simulation results. In the visualization you can:</p> <ul> <li>Load simulator memory and experiments by providing their full path</li> <li>View conversation flows and policy compliance</li> <li>Analyze agent performance and failure points</li> </ul> <p>Note: Make sure you have streamlit installed (<code>pip install streamlit</code>) before running the visualization.</p>"},{"location":"examples/education/","title":"Step-by-Step Guide to Running the simple education Chatbot","text":"<p>The education chatbot offers a basic example of a chatbot structure that operates without relying on a database or tools. For more complex examples, refer to the airline example.</p>"},{"location":"examples/education/#step-1-understand-the-input","title":"Step 1 - Understand the input","text":"<p>The <code>examples/education/input</code> folder contains the following structure:</p> <pre><code>education/\n\u2514\u2500\u2500 input/\n    \u2514\u2500\u2500 wiki.md                       # Prompt for the education agent\n</code></pre> <p>The folder contains only a path to the chatbot prompt.</p>"},{"location":"examples/education/#step-2-configure-the-simulator-run-parameters","title":"Step 2 - Configure the Simulator Run Parameters","text":"<p>The only parameter that should be updated in the configuration file is the path to the prompt file. The parameters in the configuration files have already been updated as follows in the config/config_education.yml file:</p> <pre><code>environment:\n    prompt_path: 'examples/education/input/wiki.md'  # Path to your agent's wiki/documentation\n</code></pre> <p>If you also want to modify the LLMs used by either the simulator or the chatbot, you should add the following lines to the configuration file: To change the default LLM provider or model for either the IntellAgent system or the chatbot, you can easily update the configuration file <code>config/config_edcation.yml</code>. For example adding:</p> <pre><code>llm_intellagent:\n    type: 'azure'\n\nllm_chat:\n    type: 'azure'\n    name: 'gpt-4o-mini'\n</code></pre> <p>For more configuration options, refer to the configuration guide.</p>"},{"location":"examples/education/#step-3-run-the-simulator","title":"Step 3 - Run the Simulator","text":""},{"location":"examples/education/#running-the-simulation","title":"Running the Simulation","text":"<p>Execute the simulator using:</p> <pre><code>python run.py --config ./config/config_education.yml --output_path results/education\n</code></pre> <p>This command will run generate the environment dataset (if not exists) and run the simulation with the provided chatbot prompt.</p>"},{"location":"examples/education/#step-4-run-the-simulator-visualization","title":"Step 4 - Run the Simulator Visualization","text":"<p>To visualize the simulation results using streamlit, run:</p> <pre><code>cd simulator/visualization \nstreamlit run Simulator_Visualizer.py\n</code></pre> <p>This will launch a Streamlit dashboard showing detailed analytics and visualizations of your simulation results. In the visualization you can: - Load simulator memory and experiments by providing their full path - View conversation flows and policy compliance - Analyze agent performance and failure points</p>"},{"location":"examples/education/#step-5-iterate-and-compare-results","title":"Step 5 - Iterate and compare results","text":"<p>You can now iterate and adjust the chatbot prompt or the LLM model to evaluate changes in the chatbot's performance. Once the run is complete, you can refresh the visualization to view a comparison of all the dataset experiments. </p>"},{"location":"quick_start/Running_the_Simulator/","title":"Running the Simulator","text":"<p>This guide details the steps required to run the simulator.</p>"},{"location":"quick_start/Running_the_Simulator/#1-run-the-simulator","title":"1. Run the Simulator","text":"<p>If you're utilizing Azure OpenAI services for <code>user_llm</code>, ensure you disable the default <code>jailbreak</code> filter before running the simulator.</p> <p>Use the following command to run the simulator:</p> <pre><code>python run.py --output_path &lt;output_path&gt; [--config_path &lt;config_path&gt;] [--dataset &lt;dataset&gt;]\n</code></pre> <p>Arguments: - <code>--output_path</code>: (Required) Path for saving output files - <code>--config_path</code>: (Optional) Path to config file (default: <code>config/config_default.yml</code>) - <code>--dataset</code>: (Optional) Dataset name (default: <code>'latest'</code>)</p> <p>Example:</p> <pre><code>python run.py --output_path results/airline --config_path ./config/config_airline.yml\n</code></pre> <p>Troubleshooting - Rate limit messages \u2192 Decrease <code>num_workers</code> variables in the <code>config_default</code> file. - Frequent timeout errors \u2192 Increase the <code>timeout</code> values in the <code>config_default</code> file.</p>"},{"location":"quick_start/Running_the_Simulator/#2-view-results","title":"2. View Results","text":"<p>After simulation completion, results will be organized in the following structure:</p> <pre><code>experiments/\n\u251c\u2500\u2500 dataset__[timestamp]__exp_[n]/    # Experiment run folder\n\u2502   \u251c\u2500\u2500 experiment.log                # Detailed execution logs\n\u2502   \u251c\u2500\u2500 config.yaml                   # Configuration used\n\u2502   \u251c\u2500\u2500 prompt.txt                    # Prompt template\n\u2502   \u251c\u2500\u2500 memory.db                     # Dialog memory database\n\u2502   \u2514\u2500\u2500 results.csv                   # Evaluation results\n\ndatasets/\n\u251c\u2500\u2500 dataset__[timestamp].pickle       # Dataset snapshot\n\u2514\u2500\u2500 dataset.log                       # Generation logs\n\npolicies_graph/\n\u251c\u2500\u2500 graph.log                         # Policy graph logs\n\u2514\u2500\u2500 descriptions_generator.pickle     # Generated descriptions\n</code></pre> <p>To visualize the simulation results using streamlit, run:</p> <pre><code>cd simulator/visualization \nstreamlit run Simulator_Visualizer.py\n</code></pre> <p>This will launch a Streamlit dashboard showing detailed analytics and visualizations of your simulation results. </p>"},{"location":"quick_start/configuration/","title":"Configuration","text":"<p>This guide outlines the required setup steps to configure the simulator before running it.</p>"},{"location":"quick_start/configuration/#1-configure-llm-api-keys","title":"1. Configure LLM API Keys","text":"<p>Edit the <code>config/llm_env.yml</code> file with your API credentials:</p> <pre><code>openai:\n  OPENAI_API_KEY: \"your-api-key-here\"\n  OPENAI_API_BASE: ''\n  OPENAI_ORGANIZATION: ''\n\nazure:\n  AZURE_OPENAI_API_KEY: \"your-api-key-here\"\n  AZURE_OPENAI_ENDPOINT: \"your-endpoint\"\n  OPENAI_API_VERSION: \"your-api-version\"\n</code></pre>"},{"location":"quick_start/configuration/#2-configure-simulator-parameters","title":"2. Configure Simulator Parameters","text":"<p>Before running the simulator, configure the <code>config/config_default.yml</code> file. Key configuration sections include:</p> <ul> <li><code>environment</code>: Paths for prompts, tools, and data-scheme folders</li> <li><code>description_generator</code>: Settings for flow extraction and policies</li> <li><code>event_generator</code>: LLM and worker configurations</li> <li><code>dialog_manager</code>: Dialog system and LLM settings</li> <li><code>dataset</code>: Parameters for dataset generation</li> </ul> <p>Important configuration points:</p> <ol> <li>Update file paths in the <code>environment</code> section</li> <li>Configure LLM settings (<code>type</code> and <code>name</code>)</li> <li>Adjust worker settings (<code>num_workers</code> and <code>timeout</code>)</li> <li>Set appropriate <code>cost_limit</code> values</li> </ol> <p>Key points to configure:</p> <ul> <li>Update file paths in the <code>environment</code> section to match your project structure:</li> </ul> <pre><code>prompt_path: 'examples/airline/wiki.md'  # Path to your agent's wiki/documentation\ntools_folder: 'examples/airline/tools'   # Path to your agent's tools\ndatabase_folder: 'examples/airline/data' # Path to your data schema\n</code></pre> <ul> <li>Adjust LLM configurations throughout the file:</li> </ul> <pre><code>llm:\n    type: 'openai'  # or 'azure'\n    name: 'gpt-4o'   # or other model names as gpt-4o-mini\n</code></pre> <ul> <li>Configure worker settings based on your system's capabilities. This parameter is highly important in cases of rate-limit responses:</li> </ul> <pre><code>   num_workers: 3    # Number of parallel workers\n   timeout: 10       # Timeout in seconds\n</code></pre> <ul> <li>Set appropriate cost limits to control API usage:</li> </ul> <pre><code>\ncost_limit: 30    # In dollars, for simulator dialog manager\n\n</code></pre>"},{"location":"quick_start/installation/","title":"Installation","text":"<p>This guide provides detailed instructions for setting up your development environment.</p>"},{"location":"quick_start/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>python &gt;= 3.9</code></li> </ul>"},{"location":"quick_start/installation/#step-by-step-installation","title":"Step-by-Step Installation","text":""},{"location":"quick_start/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone git@github.com:plurai-ai/intellagent.git\ncd intellagent\n</code></pre>"},{"location":"quick_start/installation/#2-install-dependencies","title":"2. Install Dependencies","text":"<p>Choose one of the following methods to install dependencies:</p>"},{"location":"quick_start/installation/#using-conda","title":"Using Conda","text":"<pre><code>conda env create -f environmen.yml\nconda activate intellagent\n</code></pre>"},{"location":"quick_start/installation/#using-pip","title":"Using pip","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"quick_start/very_quick/","title":"5 Minute Version","text":"<p>This is a '5-minute version', if you just want to run IntellAgent quickly on example data. If you want more in-depth information, go to the Installation Guide.</p>"},{"location":"quick_start/very_quick/#step-1-download-and-install","title":"Step 1 - Download and install","text":"<pre><code>git clone git@github.com:plurai-ai/intellagent.git\ncd intellagent\n</code></pre> <p>You can use Conda or pip to install the dependencies.</p> <p>Using pip: </p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"quick_start/very_quick/#step-2-set-your-llm-api-key","title":"Step 2 - Set your LLM API Key","text":"<p>Edit the <code>config/llm_env.yml</code> file to set up your LLM configuration:</p> <pre><code>openai:\n  OPENAI_API_KEY: \"your-api-key-here\"\n</code></pre> <p>To change the default LLM provider or model for either the IntellAgent system or the chatbot, you can easily update the configuration file. For instance, modify the <code>config/config_edcation.yml</code> file:</p> <pre><code>llm_intellagent:\n    type: 'azure'\n\nllm_chat:\n    type: 'azure'\n</code></pre> <p>To change the number of samples in the database you should modify the <code>num_samples</code> in the config file:</p> <pre><code>dataset:\n    num_samples: 30\n</code></pre>"},{"location":"quick_start/very_quick/#step-3-run-the-simulator","title":"Step 3 - Run the Simulator","text":"<p>If you're utilizing Azure OpenAI services for <code>user_llm</code>, ensure you disable the default <code>jailbreak</code> filter before running the simulator.</p> <p>For fast simple environment without a database, run the following command:</p> <pre><code>python run.py --output_path results/education --config_path ./config/config_education.yml \n</code></pre> <p>For more complex (slower) environment with a database, run the following command:</p> <pre><code>python run.py --output_path results/airline --config_path ./config/config_airline.yml \n</code></pre> <p>Troubleshooting - Rate limit messages \u2192 Decrease <code>num_workers</code> variables in the <code>config_default</code> file. - Frequent timeout errors \u2192 Increase the <code>timeout</code> values in the <code>config_default</code> file.</p> <p>Explore all the Customization options to configure the simulation for your environment.  or delve into the examples we provide to learn more about its capabilities.</p>"},{"location":"quick_start/very_quick/#step-4-see-the-results","title":"Step 4 - See the Results","text":"<p>To understand all the simulator generated artifacts, see the Results section.</p> <p>To visualize the simulation results using streamlit, run:</p> <pre><code>streamlit run simulator/visualization/Simulator_Visualizer.py\n</code></pre> <p>This will launch a Streamlit dashboard showing detailed analytics and visualizations of your simulation results. </p>"}]}